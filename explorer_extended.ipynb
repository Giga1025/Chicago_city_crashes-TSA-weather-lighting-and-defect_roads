{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook lets go through major topics that leads to crashes, by major i mean, weather, lightining, road defects ets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import folium\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the document to the notebook using pd.read_csv and view the first 10 elements using the .head() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Traffic_Crashes_-_Crashes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all the null values with 'N' or 0 based on if they are string or a integer(or float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "string_cols = data.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "# Fill numeric columns with 0\n",
    "data[numeric_cols] = data[numeric_cols].fillna(0)\n",
    "\n",
    "# Fill string columns with 'N'\n",
    "data[string_cols] = data[string_cols].fillna('N')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_values.sort_values(ascending=True, inplace=True)\n",
    "print(missing_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT INFO\n",
    "The data we have is from 2013 to 2024. But the data provided for the years 2013, 2014,2015 and 2025 are very low, 2, 6 9000, and 5814 respecively. These low numbers will cause inconsistancies in Seasonality plot and hinder the patterns. Hence for that we will be removing those years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CRASH_DATE'] = pd.to_datetime(data['CRASH_DATE'])\n",
    "data = data[~((data['CRASH_DATE'].dt.year == 2013) | (data['CRASH_DATE'].dt.year == 2014) | (data['CRASH_DATE'].dt.year == 2015)|(data['CRASH_DATE'].dt.year == 2025))]\n",
    "data['CRASH_DATE'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the CRASH_DATE column into datetime format which is usefull for time series analysis and visualization and create a new column for time of the crash as it will be useful for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from CRASH_DATE\n",
    "data['CRASH_DATE'] = pd.to_datetime(data['CRASH_DATE'])\n",
    "data['TIME'] = data['CRASH_DATE'].dt.time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YEARLY CRASHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yearly_data = data['CRASH_DATE'].dt.year.value_counts().sort_index()\n",
    "Yearly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.plot(Yearly_data.index,Yearly_data.values, marker = 'o')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Number of accidents')\n",
    "plt.title('Number of accidents per year')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it might seem that the crashes are less in 2016 and 2017, actually it was due to the amount of data is less comapred to the subsequent years, but once we normalize the data(Which we will do in the next cells), you will see the rate of crashes for the amount of data will similar to the other years\n",
    "\n",
    "# Explanation for the above graph\n",
    "\n",
    "A very notable trend we can observe is , the dip of crashes from 2019 to 2020, which increased later on again. But the dip is as we know, because of the COVID-19. Which decreased the traffic tremendously because of quarantine. BUT, compared to other major cities, Chicago's crash rate during the pandemic hasent fallen drastically. This was explained by the **ISSA LAW** firm on their website, \"...During the first few months of 2020, Illinois alone saw an increase of 11 percent in vehicle-related deaths.\"\n",
    "\n",
    "So, since the vehicles on road decreased, indeed there was decreasein crashes, but for those who were on road, the crashes rates increased, but this was minor number and hence the graph looks declined. There were various reasons on why the fatality rate incerased during this time\n",
    "* Motorists may be speeding since there are not as many cars on the road now.\n",
    "* Distracted driving may also be more prevalent since there is less traffic. Drivers might be practicing less caution since there are fewer cars on the road.\n",
    "* Drugs and alcohol usgaed spiked during lockdown and with the pent up desire to hit the road, the drivers take vehicles on road while under substance, which as we know impaires their thinking process and response time, inevitabilty leading to accidents.\n",
    "\n",
    "So this explains why even during the time when pandemic was at its core (2020-2021), the crashes in chicago were increasing, but slowly. That was until early 2021, from which they were controlled with stricter rules and regulations. \n",
    "\n",
    "The case study chicago during the pandemic is a very interesting one, as every major city had their accident rates hit the record low during the during of pandemic, while chicago had their all time highest fatality rate during the pandemic. Below are the few links for reference.\n",
    "\n",
    "* [NHTSA: 2020 Had The Most Fatal Crashes Since ’07](https://www.coplancrane.com/posts/2020-had-most-fatal-crashes-since-07/)\n",
    "* [Why Are Illinois Vehicle Fatality Rates Up During the Pandemic?](https://www.issalawoffices.com/personal-injury-criminal-law-blog/illinois-vehicle-fatality-rates-pandemic)\n",
    "* [Average Chicagoan Spent 102 Hours Stuck In Traffic Last Year — Among Worst Gridlock In The World](https://blockclubchicago.org/2025/01/06/chicago-has-2nd-worst-traffic-in-the-world-with-average-driver-spending-102-hours-gridlocked-study/#:~:text=The%20bumper%2Dto%2Dbumper%20bump,year%2C%20according%20to%20the%20study.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_c = pd.DataFrame(data['CRASH_DATE'])\n",
    "crashes_c['CRASH_DATE'] = pd.to_datetime(crashes_c['CRASH_DATE'], errors='coerce')\n",
    "crashes = crashes_c.groupby(crashes_c['CRASH_DATE'].dt.date).size()\n",
    "crashes.index= pd.to_datetime(crashes.index)\n",
    "crashes = crashes.reset_index()\n",
    "crashes = crashes.set_index('CRASH_DATE')\n",
    "crashes=crashes.rename(columns = {0:'COUNT'})\n",
    "\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "cubehelix_palette = sns.color_palette(\"cubehelix\", 8)  \n",
    "\n",
    "plot = crashes.plot(y='COUNT', figsize=(15, 5), color=cubehelix_palette[0])\n",
    "plot.set_xlabel(\"Year\")\n",
    "plot.set_ylabel(\"Crashes\")\n",
    "plot.set_title(\"Crashes per day\")\n",
    "plot.legend([\"Crashes\"])\n",
    "plot.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph is similar to the previous graph we went through, but we are ploting the graph day wise. But unlike the previous graph(which was very smooth), this one is very noise, and that is to be expected as plotting for individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Count the number of crashes per day, group by CRASH DATE\n",
    "daily_crashes = crashes\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"darkgrid\")\n",
    "cubehelix_palette = sns.color_palette(\"cubehelix\", 8)  # Generate 8 colors from the cubehelix palette\n",
    "\n",
    "# Plot the daily crashes time series\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_crashes, label='Daily crashes', color=cubehelix_palette[0])  # Use palette color\n",
    "plt.title('Daily Motor Vehicle Collisions in NYC', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Crashes', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Decompose the time series\n",
    "decomposition = seasonal_decompose(daily_crashes, model='additive', period=365)\n",
    "\n",
    "# Plot the decomposed components\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
    "decomposition.trend.plot(ax=ax1, color=cubehelix_palette[1])  # Use palette color\n",
    "ax1.set_title('Trend', fontsize=14)\n",
    "ax1.grid(alpha=0.5)\n",
    "\n",
    "decomposition.seasonal.plot(ax=ax2, color=cubehelix_palette[2])  # Use palette color\n",
    "ax2.set_title('Seasonality', fontsize=14)\n",
    "ax2.grid(alpha=0.5)\n",
    "\n",
    "decomposition.resid.plot(ax=ax3, color=cubehelix_palette[3])  # Use palette color\n",
    "ax3.set_title('Residuals', fontsize=14)\n",
    "ax3.grid(alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze residuals for significant fluctuations\n",
    "residuals = decomposition.resid\n",
    "\n",
    "# Calculate mean and standard deviation of the residuals (ignoring NaNs)\n",
    "mean_resid = np.nanmean(residuals)\n",
    "std_resid = np.nanstd(residuals)\n",
    "\n",
    "# Define threshold for significant fluctuations (e.g., ±2 standard deviations)\n",
    "upper_threshold = mean_resid + 2 * std_resid\n",
    "lower_threshold = mean_resid - 2 * std_resid\n",
    "\n",
    "# Find dates with significant fluctuations\n",
    "significant_fluctuations = residuals[(residuals > upper_threshold) | (residuals < lower_threshold)]\n",
    "\n",
    "# Output the significant dates and their residual values\n",
    "print(\"Significant Fluctuations Detected:\")\n",
    "print(significant_fluctuations)\n",
    "\n",
    "# Plot residuals with highlighted significant points\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(residuals.index, residuals, label='Residuals', color=cubehelix_palette[0])  # Use palette color\n",
    "plt.axhline(upper_threshold, color=cubehelix_palette[4], linestyle='--', label='Upper Threshold (+2σ)')  # Use palette color\n",
    "plt.axhline(lower_threshold, color=cubehelix_palette[4], linestyle='--', label='Lower Threshold (-2σ)')  # Use palette color\n",
    "plt.scatter(significant_fluctuations.index, significant_fluctuations, color=cubehelix_palette[5], label='Significant Fluctuations', zorder=5)  # Use palette color\n",
    "plt.title('Residuals with Significant Fluctuations Highlighted', fontsize=16)\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Residuals', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(cubehelix_palette)\n",
    "plt.title(\"Cubehelix Palette\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_fluctuations.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes[crashes.index == '2020-03-22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES ANALYSIS\n",
    "\n",
    "What we did above is called analysis, we usually use this technique when we have some data that changes with time, and exhibit patterns accordingly. In this instance we are finding out three of the most important patters:\n",
    "* Trend: This gives us info on how the data changes over the year, but in a linear way.\n",
    "* Seasonality: Shows how the data behaves in the time frame it is in. The seasonalities keep repeating at regular intervals.\n",
    "* Redisual: These are the unpredictable patterns and outliers, which the trend and seasonality couldnt make a understading of. So we can now only use machin learning or deep learning models to make aprediction for these data and once predicted, add back the seasonality and trends to make a proper prediction.\n",
    "\n",
    "## Understanding the plots\n",
    "\n",
    "###  Trend\n",
    "As seen above, the crashes drasticaly from 2016, which was not suprising, in illinois there were 1,082 fatalies (second time to cross 1000 since 2008), this was due a lot of factors, increased drivers, no seat belt, drunk and drive etc, so chicago also had contribution to that, with a total of 113 deaths. This was followed up by a increase of fatalities to 133 in 2017, an 18% increase, but this was then controlled in 2018, the gov of chicago, as revised their [VISION ZERO CHICAGO](https://activetrans.org/our-work/walking/vision-zero/) plan, which included more advanced traffic equipment, enhancing child safety zoen etc and since then the crashes had remained constant, until the pandamic, where it dropped until 2020, rose again, stayed stable since then(We have discucced about the trend during the pandamic in the previous section)\n",
    "\n",
    "Few links for your understadning:\n",
    "* [1,082 People Died in Illinois Car Accidents in 2016. Here’s Why](https://www.sgklawyers.com/blog/2018/06/car-accidents-fatality/)\n",
    "* [Chicago Traffic Accident Deaths Rise as Fatalities Nationwide Top 40K](https://www.cooneyconway.com/blog/chicago-traffic-deaths-rise-fatalities-nationwide-top-40k)\n",
    "\n",
    "\n",
    "### Seasonality\n",
    "As we can see from the above plots, the highest number of accidents annually are in the month of October, This was due to a lot of days in october being rainy(stats are provided in the link below), and more ever it is when is almost the start of winter, so with a mix of wet and cold climate, its very tricky for drivers and on the opposite side of the spectrum, the lowest accidents annually are in the late december, not to suprise, as this during the winter brake, and a lot people tend to stay home and enjoy the christmas and new year eve with hot chocolate!\n",
    "\n",
    "links:\n",
    "* [When Do Most Car Accidents Occur in Chicago?](https://www.dopplr.com/when-do-most-car-accidents-occur-in-chicago-2016-2020/)\n",
    "\n",
    "### Residual\n",
    "\n",
    "These are the irregularities which the Trend and seasonality were not able pick up the patterns from. Out of all random spikes, lets look at the highest positive spike(more crashes than upper threshold) and highest negative spike(Less crashes than the lower threshold).\n",
    "\n",
    "***Highest positive spike:*** The highest positive spike was on 2019-01-12, on this day a total of 583 happened, this is to be blamed on the snow stormed that happened on that day and carried on to 13th, further more, there were many fatality accidents, Driver crashing into a state trooper, 35-years old man head on collision, baby's death during a U-turn crash etc. All this factors had made it very unpredictable tobe picked up by either the trend and seasonality patterns. \n",
    "\n",
    "links: [Winter storm results in nearly 300 reported traffic crashes](https://www.theintelligencer.com/news/article/Winter-storm-results-in-nearly-300-reported-13530315.php)\n",
    "\n",
    "***Highest negative spike:*** The highest negative spike was on 2020-03-22 with a accidents numbering to 96. This day was a nightmare with combination of humungous increase in the corona cases in chicago and heavy snow fall in north illinois. These two reasons can explain as to why there were just 96 crashes on this day.\n",
    "\n",
    "links: \n",
    "* [March 22-23, 2020: Late Season Moderate to Heavy Wet Snow Event](https://www.weather.gov/lot/2020mar2223_snow)\n",
    "* [Coronavirus in Illinois updates: Here’s what happened March 21-22 with COVID-19 in the Chicago area](https://www.chicagotribune.com/2020/03/22/coronavirus-in-illinois-updates-heres-what-happened-march-21-22-with-covid-19-in-the-chicago-area/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEATHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = data['WEATHER_CONDITION'].value_counts()\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.DataFrame(weather)\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the \"CLEAR\" weather is the winner by a huge margine here, with a whopping 7,04,941 accidents, that is 78.6% of all the crashes happened on a clear day! But, we cant come to the conclusion so early, because out of the year, a lot of days have clear skies, unlike snow and rainy weather, which are season specific. So lets try to normalize the data and see, because, there are weather that happen only few months a year, but results in a lot of accidents. \n",
    "\n",
    "Before that lets just have a look at the statistics before normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dot plot\n",
    "sns.set(style=\"darkgrid\")\n",
    "cubehelix_palette = sns.color_palette(\"cubehelix\", 8)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(weather_df.index,weather_df['count'], color='blue', s=100, edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Proportion (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Weather Condition')\n",
    "plt.title('Proportion of Accidents by Weather Condition')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to remember that the the dots after the \"OTHER\" laber are not 0, but some numerical. The upper bound for the category is so high that the less occuring events looks almost negligible. lets normalize before to atleast get rid of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CRASH_DATE is a datetime column\n",
    "data['CRASH_DATE'] = pd.to_datetime(data['CRASH_DATE'], errors='coerce')\n",
    "\n",
    "# Extract only the date\n",
    "data['CRASH_DATE_ONLY'] = data['CRASH_DATE'].dt.date\n",
    "\n",
    "# Group by WEATHER_CONDITION\n",
    "weather_grouped = data.groupby('WEATHER_CONDITION').agg(\n",
    "    accidents_count=('CRASH_RECORD_ID', 'count'),  # Count of crashes\n",
    "    days_of_weather=('CRASH_DATE_ONLY', 'nunique')  # Unique crash days\n",
    ").reset_index()\n",
    "\n",
    "# Calculate normalized rate\n",
    "weather_grouped['normalized_rate'] = (\n",
    "    weather_grouped['accidents_count'] / weather_grouped['days_of_weather']\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Total crashes by weather condition\n",
    "ax[0].bar(weather_grouped['WEATHER_CONDITION'], weather_grouped['accidents_count'])\n",
    "ax[0].set_title('Total Crashes by Weather Condition')\n",
    "ax[0].set_ylabel('Number of Crashes')\n",
    "ax[0].set_xticklabels(weather_grouped['WEATHER_CONDITION'], rotation=45, ha='right')\n",
    "\n",
    "# Normalized crash rates\n",
    "ax[1].bar(weather_grouped['WEATHER_CONDITION'], weather_grouped['normalized_rate'])\n",
    "ax[1].set_title('Normalized Crash Rate by Weather Condition')\n",
    "ax[1].set_ylabel('Crashes per Day')\n",
    "ax[1].set_xticklabels(weather_grouped['WEATHER_CONDITION'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we observed from the above graph:\n",
    "* Suprisingly the number of accidents on \"CLEAR\" day seems to triumph over the others even after normalization. This was explained in a [reserach paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC1449863/), \"Crash counts are not inevitably higher in snowy weather than in dry weather. On the one hand, snow makes driving more dangerous, by reducing tire adherence and impairing visibility. On the other hand, experienced drivers typically drive more slowly and carefully in snowy weather, and many people avoid or postpone unnecessary travel.\" Well, atleast the chicago drivers being careful here!\n",
    "* The accident occurances during the \"SNOW\" days had raisen over \"RAIN\" accidents, depicting that, accidents tend to happen more often during snowy days than rainy.\n",
    "* The other categories sure had raisen a little but nothing major."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lighting = data['LIGHTING_CONDITION'].value_counts()\n",
    "lighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.bar(lighting.index,lighting.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Lighting Condition')\n",
    "plt.ylabel('Number of accidents')\n",
    "plt.title('Number of accidents per lighting condition')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accidents during \"DAYLIGHT\" seems to happend thrice thats happening during the \"DARKNESS, LIGHTED ROAD\" (Night). Which is not a suprise, as the city activity hours during the day is quite higher than night. But, then again we can normalize here as for only a few hours of active time at night, there were 196763 accidents, but we dont have data on how many vehicles are on road on the respective time, hence normalization here is not possible. The distinction here looks very reasonable, so lets proceed with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOW, lets see how WEATHER and LIGHTINING have a combined effect on accidents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pivot table\n",
    "heatmap_data = data.pivot_table(index='WEATHER_CONDITION', columns='LIGHTING_CONDITION', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Calculate the percentage for each cell\n",
    "total = heatmap_data.sum().sum()  # Total crashes\n",
    "percentage_data = heatmap_data / total * 100  # Convert to percentages\n",
    "\n",
    "# Calculate row and column totals\n",
    "row_totals = percentage_data.sum(axis=1)  # Row-wise totals\n",
    "col_totals = percentage_data.sum(axis=0)  # Column-wise totals\n",
    "\n",
    "# Add the totals as a new row and column\n",
    "percentage_data_with_totals = percentage_data.copy()\n",
    "percentage_data_with_totals.loc['Total'] = col_totals\n",
    "percentage_data_with_totals['Total'] = pd.concat([row_totals, pd.Series(col_totals.sum(), index=['Total'])])\n",
    "\n",
    "# Combine raw counts and percentages for annotations\n",
    "annot_data = heatmap_data.astype(str) + \"\\n(\" + percentage_data.round(2).astype(str) + \"%)\"\n",
    "annot_data.loc['Total'] = col_totals.round(2).astype(str) + \"%\"\n",
    "annot_data['Total'] = pd.concat([row_totals.round(2).astype(str) + \"%\", pd.Series(\"100%\", index=['Total'])])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    percentage_data_with_totals, \n",
    "    annot=annot_data, \n",
    "    fmt=\"\",  # Let the formatted annotations handle display\n",
    "    cmap=\"YlGnBu\", \n",
    "    linewidths=0.5, \n",
    "    linecolor=\"gray\", \n",
    "    cbar_kws={\"label\": \"Percentage of Total Crashes\"}\n",
    ")\n",
    "\n",
    "# Enhance labels and title\n",
    "plt.title(\n",
    "    \"Crashes by Weather and Lighting Conditions\\n(Note: Percentages rounded to two decimal places)\", \n",
    "    fontsize=16, pad=20\n",
    ")\n",
    "plt.xlabel(\"Lighting Condition\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Weather Condition\", fontsize=14, labelpad=10)\n",
    "plt.xticks(fontsize=12, rotation=45, ha=\"right\")\n",
    "plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heat map is the best visualization for this kind of relations, and without disappointing it delivered us with perfect information. \n",
    "\n",
    "What we understood from the plot.\n",
    "* Tons of accidents happen on a CLEAR day during the DAYLIGHT! i wouldnt have belived this without this anlysis, but number dont lie, and following, other big number accidents seems to be happened on CLEAR days too.\n",
    "* More accidents happen on a CLOUDY day during the DAYLIGHT than SNOW day during daylight. Now this is unexpected for me too, since anyone would believe more accident to happen on snowy days!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRASHES(With Geospatial visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ROAD_DEFECT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street = data[['STREET_NAME', 'STREET_NO', 'CRASH_DATE', 'LONGITUDE', 'LATITUDE']]\n",
    "street.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "street = street[~((street['LATITUDE'] == 0) & (street['LONGITUDE'] == 0))]\n",
    "street.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_count = street['STREET_NAME'].value_counts()\n",
    "street_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import pandas as pd\n",
    "\n",
    "data_geo = street.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "\n",
    "# Center the map around the mean latitude and longitude\n",
    "map_center = [data_geo['LATITUDE'].mean(), data_geo['LONGITUDE'].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "# Prepare heatmap data\n",
    "heat_data = [[row['LATITUDE'], row['LONGITUDE']] for _, row in data_geo.iterrows()]\n",
    "\n",
    "# Create the heatmap\n",
    "HeatMap(heat_data, radius=8, max_zoom=13).add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "#m.save(\"Heatmap.html\")     #Uncomment thid line to save the map as a html file\n",
    "#m          #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the city of chicago!\n",
    "\n",
    "One notable thing at a glance, is how densed up the crashes were at the Downtown (Near north side, near south side and new eastside), This is to be expected, as this is the most important parts of the chicago city. First of all, The loop, the loop is THE downtown, it contains a lot skycrapers with a lot of tech companies, similar to manhatten at nyc(manhatten is better ;) ) This expalins the happening of accidents for such a small area.\n",
    "\n",
    "For not a suprise, we can see a LOT of accidents happen at or close to intersections, which is to be expected in major cities\n",
    "\n",
    "* Western avenue: Western avenue is the biggest street in chicago, so, bigger street means, a lot of intersections, which in turn leads to more accidents. For reference, only in 2024, there were 59 series crashes in the street. \n",
    "\n",
    "* Cicero avenue: Cicero avenue is the gateway for people entering from the midway airport and it is one of the most used truck routes in the county.\n",
    "\n",
    "The main pattern is, more intercetion a street has, more accidents tend to occur.\n",
    "\n",
    "Links: [Most Dangerous Roads in Chicago, IL](https://www.makeroadssafe.org/most-dangerous-roads-in-chicago-il/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets have a look at the areas more prone to accidents during snowy weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snow = data[(data[\"WEATHER_CONDITION\"] == \"SNOW\") | (data[\"WEATHER_CONDITION\"] == \"BLOWING SNOW\")]\n",
    "data_snow = data_snow[[\"CRASH_DATE\", \"WEATHER_CONDITION\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "data_snow.reset_index(drop=True, inplace=True)\n",
    "data_snow['WEATHER_CONDITION'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter such that we only take the crashes that happened on a \"SNOW\" and \"BLOWING SNOW\" day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Filter out rows with missing coordinates\n",
    "data_geo = data_snow.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n",
    "\n",
    "# Create a base map\n",
    "base_map = folium.Map(location=[41.8781, -87.6298], zoom_start=10)  # Chicago as an example\n",
    "\n",
    "# Define colors for the two weather conditions\n",
    "color_mapping = {\n",
    "    \"SNOW\": \"blue\",\n",
    "    \"BLOWING SNOW\": \"red\"\n",
    "}\n",
    "\n",
    "# Create separate marker clusters for each weather condition\n",
    "for condition, color in color_mapping.items():\n",
    "    condition_group = folium.FeatureGroup(name=condition, show=True).add_to(base_map)\n",
    "    condition_cluster = MarkerCluster().add_to(condition_group)\n",
    "    \n",
    "    # Add markers for the specific condition\n",
    "    for _, row in data_geo[data_geo[\"WEATHER_CONDITION\"] == condition].iterrows():\n",
    "        folium.Marker(\n",
    "            location=[row[\"LATITUDE\"], row[\"LONGITUDE\"]],\n",
    "            popup=f\"Date: {row['CRASH_DATE']}, Condition: {row['WEATHER_CONDITION']}\",\n",
    "            icon=folium.Icon(color=color)\n",
    "        ).add_to(condition_cluster)\n",
    "\n",
    "# Add layer control to switch between groups\n",
    "folium.LayerControl().add_to(base_map)\n",
    "\n",
    "# Save or display the map\n",
    "#base_map.save(\"snow_accidents_cluster_colored.html\")     #Uncomment thid line to save the map as a html file\n",
    "#base_map         #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To not our suprise, most of the accidents during snow near the downtown chicago. Especially since its such a dense area, even a milli second delay in breaking can lead to minor crashes.\n",
    "\n",
    "More ever we have to observe that a lot of crashes tend to happen near or around th elivated bridges. This can be at the entries or exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_snow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map\n",
    "base_map = folium.Map(location=[41.8781, -87.6298], zoom_start=10)  # Chicago coordinates\n",
    "\n",
    "# Prepare data for heatmap (remove invalid coordinates)\n",
    "heat_data = data_snow[(data_snow[\"LATITUDE\"] != 0) & (data_snow[\"LONGITUDE\"] != 0)]\n",
    "heat_data = heat_data[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()\n",
    "\n",
    "# Add heatmap to the base map\n",
    "HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(base_map)\n",
    "\n",
    "# Save or display the map\n",
    "#base_map.save(\"snow_accidents_heatmap.html\")    #Uncomment thid line to save the map as a html file\n",
    "#base_map        #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFECTS ON ROAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects = data[['ROAD_DEFECT','CRASH_DATE','LONGITUDE','LATITUDE', 'STREET_NAME']]\n",
    "defects = defects[(defects[\"LATITUDE\"] != 0) & (defects[\"LONGITUDE\"] != 0)]\n",
    "\n",
    "defects.reset_index(drop=True, inplace=True)\n",
    "defects_count = defects['ROAD_DEFECT'].value_counts()\n",
    "defects['ROAD_DEFECT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,6))\n",
    "plt.bar(defects_count.index,defects_count.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Road Defect')\n",
    "plt.ylabel('Number of accidents')\n",
    "plt.title('Number of accidents per road defect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a good thing that a lot of accidents are occuring due to human errors and not the infrastructre defects, but there are few cases where the road defects had led to minor or major crashes, so lets go thought those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with LATITUDE or LONGITUDE equal to 0\n",
    "zero_coordinates = defects[(defects[\"LATITUDE\"] == 0) | (defects[\"LONGITUDE\"] == 0)]\n",
    "\n",
    "# Display rows with zero coordinates\n",
    "print(f\"Number of rows with zero coordinates: {len(zero_coordinates)}\")\n",
    "print(zero_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects_valid = defects[~((defects['ROAD_DEFECT']== 'UNKNOWN') | (defects['ROAD_DEFECT']== 'OTHER') | (defects['ROAD_DEFECT']== 'NO DEFECTS'))]\n",
    "defects_valid.reset_index(drop=True, inplace=True)\n",
    "defects_valid_count = defects_valid['ROAD_DEFECT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(defects_valid_count.index, defects_valid_count.values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding labels and title with enhancements\n",
    "plt.xticks(rotation=45, fontsize=12, ha='right',fontweight='bold')\n",
    "plt.xlabel('Road Defect', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Number of Accidents', fontsize=14,fontweight='bold')\n",
    "plt.title('Number of Accidents per Road Defect', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Adding gridlines for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjusting layout for better fit\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.scatter(defects_valid['LONGITUDE'], defects_valid['LATITUDE'], alpha=0.5, c='r', s=1)\n",
    "plt.title('Accidents by Road Defects')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.annotate('Ashland Ave', xy=(-87.673, 41.86), xytext=(-87.57, 41.86),  arrowprops=dict(facecolor='blue', shrink=0.05, width=1, headwidth=6), fontsize=12,color='blue')\n",
    "plt.annotate('Western Ave', xy=(-87.687, 41.90), xytext=(-87.7, 42),  arrowprops=dict(facecolor='blue', shrink=0.05, width=1, headwidth=6), fontsize=12,color='blue')\n",
    "plt.annotate('Lake shore dr NB', xy=(-87.652, 41.97), xytext=(-87.6, 42),  arrowprops=dict(facecolor='blue', shrink=0.05, width=1, headwidth=6), fontsize=12,color='blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph vaguely looks like the chicago city, because it is! The red dots are the accidents that happened due road_defects,and If u look carefully, we can see the crashes in the weastern, Ash land Avenue , Lake shore dr NB. I think you can guess who will top our list.\n",
    "\n",
    "The road defects with lake shore drive especially is very major, as pot holes as large as manholes causing heavy damage to the vehicles,multiple vehicles suffered flat tires which inevitably led to minor crashes.\n",
    "\n",
    "links: [Crews work to repair stretch of DuSable Lake Shore Drive after cars damaged by potholes](https://abc7chicago.com/post/chicago-traffic-cdot-crews-repair-stretch-dusable-lake-shore-drive-after-cars-damaged-potholes-belmont-avenue/15329317/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_10 = defects_valid.groupby('STREET_NAME').size().sort_values(ascending=False)\n",
    "Top_10.nunique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have a lot of streets, lets take a look at the top 10 streets where accidents were caused by road defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_streets = Top_10[Top_10>= 139].index\n",
    "filtered_defects_top_10 = defects_valid[defects_valid['STREET_NAME'].isin(valid_streets)].reset_index(drop=True)\n",
    "filtered_defects_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Create a base map\n",
    "base_map = folium.Map(location=[41.8781, -87.6298], zoom_start=10)  # Chicago coordinates\n",
    "\n",
    "# Prepare data for heatmap (remove invalid coordinates)\n",
    "heat_data = filtered_defects_top_10[(filtered_defects_top_10[\"LATITUDE\"] != 0) & (filtered_defects_top_10[\"LONGITUDE\"] != 0)]\n",
    "heat_data = heat_data[[\"LATITUDE\", \"LONGITUDE\"]].values.tolist()\n",
    "\n",
    "# Add heatmap to the base map\n",
    "HeatMap(heat_data, radius=10, blur=15, max_zoom=1).add_to(base_map)\n",
    "\n",
    "# Save or display the map\n",
    "#base_map.save(\"snow_accidents_heatmap.html\")   #Uncomment thid line to save the map as a html file\n",
    "#base_map       #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you zoom in a little, we can see how the major road defects are only caused in the main streets, this mainly due to heavy usage cars, trucks, buses etc. Unlike connecting streets, which wont be used as much by heavy duety vehicles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crashes overtime due to defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects_valid['ROAD_DEFECT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_d = defects_valid[['ROAD_DEFECT','CRASH_DATE']]\n",
    "crashes_d['CRASH_DATE'] = pd.to_datetime(crashes_d['CRASH_DATE']).dt.year\n",
    "crashes_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_dy = crashes_d.groupby('CRASH_DATE').count()\n",
    "crashes_dy = pd.DataFrame(crashes_dy)\n",
    "crashes_dy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(crashes_dy.index,crashes_dy['ROAD_DEFECT'], marker = 'o')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Crashes due to defects')\n",
    "plt.title('Crashes due to defects over the years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_holes = crashes_d[crashes_d['ROAD_DEFECT']=='RUT, HOLES']\n",
    "crashes_holes.reset_index(drop=True,inplace=True)\n",
    "crashes_holes_y = crashes_holes.groupby('CRASH_DATE').count()\n",
    "crashes_holes_y = pd.DataFrame(crashes_holes_y)\n",
    "\n",
    "# worn surface\n",
    "crashes_worn = crashes_d[crashes_d['ROAD_DEFECT']=='WORN SURFACE']\n",
    "crashes_worn.reset_index(drop=True,inplace=True)\n",
    "crashes_worn_y = crashes_worn.groupby('CRASH_DATE').count()\n",
    "crashes_worn_y = pd.DataFrame(crashes_worn_y)\n",
    "\n",
    "#Debris\n",
    "crashes_deb = crashes_d[crashes_d['ROAD_DEFECT']=='DEBRIS ON ROADWAY']\n",
    "crashes_deb.reset_index(drop=True,inplace=True)\n",
    "crashes_deb_y = crashes_deb.groupby('CRASH_DATE').count()\n",
    "crashes_deb_y = pd.DataFrame(crashes_deb_y)\n",
    "\n",
    "#shoulder\n",
    "crashes_sh = crashes_d[crashes_d['ROAD_DEFECT']=='SHOULDER DEFECT']\n",
    "crashes_sh.reset_index(drop=True,inplace=True)\n",
    "crashes_sh_y = crashes_sh.groupby('CRASH_DATE').count()\n",
    "crashes_sh_y = pd.DataFrame(crashes_sh_y)\n",
    "\n",
    "#PLOT\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2,figsize = (20,8))\n",
    "ax[0,0].plot(crashes_holes_y.index, crashes_holes_y['ROAD_DEFECT'],color='red',marker = 'o')\n",
    "ax[0,0].set_title('Pot holes')\n",
    "ax[0,0].set_ylabel('Crashes')\n",
    "ax[0,0].legend()\n",
    "\n",
    "# Plot on the second subplot (axs[1])\n",
    "ax[0,1].plot(crashes_worn_y.index, crashes_worn_y['ROAD_DEFECT'], color='orange', marker = 'o')\n",
    "ax[0,1].set_title('Worn surfaces')\n",
    "ax[0,1].set_ylabel('Crashes')\n",
    "ax[0,1].legend()\n",
    "\n",
    "ax[1,0].plot(crashes_deb_y.index, crashes_deb_y['ROAD_DEFECT'],color='green',marker = 'o')\n",
    "ax[1,0].set_title('Debris on road')\n",
    "ax[1,0].set_xlabel('Years')\n",
    "ax[1,0].set_ylabel('Crashes')\n",
    "ax[1,0].legend()\n",
    "\n",
    "# Plot on the second subplot (axs[1])\n",
    "ax[1,1].plot(crashes_sh_y.index, crashes_sh_y['ROAD_DEFECT'],marker = 'o')\n",
    "ax[1,1].set_title('Shoulder defect')\n",
    "ax[1,1].set_xlabel('Years')\n",
    "ax[1,1].set_ylabel('Crashes')\n",
    "ax[1,1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter for 'WORN SURFACE' road defects\n",
    "area_worn = defects_valid[defects_valid['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "\n",
    "# Group by 'STREET_NAME' and count occurrences\n",
    "worn_street_counts = area_worn['STREET_NAME'].value_counts()\n",
    "\n",
    "# Select the top 10 streets with the highest counts\n",
    "top_10_worn_streets = worn_street_counts.head(40).index.tolist()\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "num_streets = len(top_10_worn_streets)\n",
    "num_cols = 2\n",
    "num_rows = (num_streets + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten in case of a 2D array\n",
    "\n",
    "# Iterate over the top streets and corresponding axes\n",
    "for i, (street, ax) in enumerate(zip(top_10_worn_streets, axes)):\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    \n",
    "    # Sort data by 'CRASH_DATE'\n",
    "    street_data['CRASH_DATE'] = street_data['CRASH_DATE'].dt.year\n",
    "    street_g = street_data.groupby('CRASH_DATE').count()\n",
    "\n",
    "    # Plot the number of crashes over time\n",
    "    ax.plot(street_g.index, street_g.values, marker='o', linestyle='-')\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f'Crashes on {street} Over Time')\n",
    "    ax.set_xlabel('Crash Date')\n",
    "    ax.set_ylabel('Cumulative Number of Crashes')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "area_worn = defects_valid[defects_valid['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "\n",
    "# Group by 'STREET_NAME' and count occurrences\n",
    "worn_street_counts = area_worn['STREET_NAME'].value_counts()\n",
    "\n",
    "# Select the top 40 streets with the highest counts\n",
    "top_10_worn_streets = worn_street_counts.head(856).index.tolist()\n",
    "\n",
    "# Initialize a dictionary to store differences\n",
    "positive_differences = {}\n",
    "name = {}\n",
    "\n",
    "# Iterate over the top streets\n",
    "for street in top_10_worn_streets:\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    street_data['CRASH_YEAR'] = street_data['CRASH_DATE'].dt.year\n",
    "    \n",
    "    # Group by year and count crashes\n",
    "    street_g = street_data.groupby('CRASH_YEAR').size()\n",
    "    \n",
    "    # Ensure both 2023 and 2024 exist in the data\n",
    "    if 2023 in street_g.index and 2024 in street_g.index:\n",
    "        difference = street_g.loc[2024] - street_g.loc[2023]\n",
    "        if difference > 0:\n",
    "            positive_differences[street] = difference\n",
    "            name[street] = street\n",
    "\n",
    "# Create a Geo Map for streets with positive differences\n",
    "geo_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)  # Centered on Chicago (example location)\n",
    "\n",
    "# Add proportional circles for streets with positive differences\n",
    "for street, difference in positive_differences.items():\n",
    "    # Filter the location data for the street for 2024\n",
    "    location_data_2024 = area_worn[\n",
    "        (area_worn['STREET_NAME'] == street) & \n",
    "        (area_worn['CRASH_DATE'].dt.year == 2024)\n",
    "    ]\n",
    "    \n",
    "    # Use the median latitude and longitude for better accuracy\n",
    "    lat = location_data_2024['LATITUDE'].median()\n",
    "    lon = location_data_2024['LONGITUDE'].median()\n",
    "    \n",
    "    # Add a proportional circle marker to the map\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=int(difference) * 4,  # Convert difference to a standard int\n",
    "        tooltip=f\"Street: {street}\",  # Label the street name\n",
    "        popup=f\"{street}: {difference} more crashes in 2024\",  # Additional details in popup\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='blue',\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(geo_map)\n",
    "\n",
    "# Save the map to an HTML file or display it\n",
    "#geo_map.save(\"proportional_crash_differences_with_labels.html\")  #Uncomment thid line to save the map as a html file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Provide a download link for the HTML file\n",
    "#display(FileLink(\"proportional_crash_differences_with_labels.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#geo_map       #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'CRASH_DATE' is in datetime format\n",
    "defects_valid['CRASH_DATE'] = pd.to_datetime(defects_valid['CRASH_DATE'])\n",
    "\n",
    "# Filter for streets in the 'name' list and for the year 2024\n",
    "filtered_2024_data = defects_valid[\n",
    "    (defects_valid['STREET_NAME'].isin(name.keys())) &\n",
    "    (defects_valid['CRASH_DATE'].dt.year == 2024)\n",
    "]\n",
    "\n",
    "filtered_2024_data_worn = filtered_2024_data[filtered_2024_data['ROAD_DEFECT']=='WORN SURFACE']\n",
    "filtered_2024_data_worn.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "\n",
    "# Define the center of the map (e.g., Chicago) based on the mean latitude and longitude\n",
    "map_center_lat = filtered_2024_data_worn['LATITUDE'].mean()\n",
    "map_center_lon = filtered_2024_data_worn['LONGITUDE'].mean()\n",
    "\n",
    "# Create a folium map centered on the data\n",
    "heat_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12)\n",
    "\n",
    "# Prepare data for the heat map\n",
    "heat_data = filtered_2024_data_worn[['LATITUDE', 'LONGITUDE']].values.tolist()\n",
    "\n",
    "# Add a heat map layer\n",
    "HeatMap(heat_data).add_to(heat_map)\n",
    "\n",
    "# Save the heat map to an HTML file\n",
    "#heat_map.save(\"filtered_worn_surface_2024_heatmap.html\")   #Uncomment thid line to save the map as a html file\n",
    "\n",
    "# Provide a link to download the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"filtered_worn_surface_2024_heatmap.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#heat_map      #Uncomment this line to display the map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Create a map centered at the calculated mean latitude and longitude\n",
    "proportional_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12)\n",
    "\n",
    "# Ensure 'CRASH_COUNT' column exists\n",
    "filtered_2024_data_worn['CRASH_COUNT'] = 1\n",
    "\n",
    "# Group by coordinates and calculate crash counts\n",
    "crash_counts = filtered_2024_data_worn.groupby(['LATITUDE', 'LONGITUDE']).size().reset_index(name='CRASH_COUNT')\n",
    "\n",
    "# Function to determine marker color based on crash count\n",
    "# Ensure CRASH_COUNT is numeric\n",
    "crash_counts['CRASH_COUNT'] = crash_counts['CRASH_COUNT'].astype(int)\n",
    "\n",
    "# Define a function to determine the marker color\n",
    "def get_color(crash_count):\n",
    "    if crash_count == 1:\n",
    "        return 'red'  # Red for 1 crash\n",
    "    elif crash_count == 2:\n",
    "        return 'blue'  # Blue for 2 crashes\n",
    "    return 'gray'  # Default color (in case of unexpected values)\n",
    "\n",
    "# Create proportional markers with dynamic colors\n",
    "for _, row in crash_counts.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        radius=row['CRASH_COUNT'] * 5,  # Scale circle size for better visibility\n",
    "        popup=f\"<b>Crashes:</b> {row['CRASH_COUNT']}\",  # HTML-styled popup\n",
    "        tooltip=f\"Crashes: {row['CRASH_COUNT']}\",  # Tooltip for hover\n",
    "        color='black',  # Circle border color\n",
    "        fill=True,\n",
    "        fill_color=get_color(row['CRASH_COUNT']),  # Dynamic fill color\n",
    "        fill_opacity=0.8,  # Enhanced opacity for better visibility\n",
    "    ).add_to(proportional_map)\n",
    "legend_html = \"\"\"\n",
    "<div style=\"\n",
    "position: fixed;\n",
    "bottom: 50px;\n",
    "left: 50px;\n",
    "width: 350px;\n",
    "background-color: white;\n",
    "border: 2px solid black;\n",
    "z-index: 1000;\n",
    "padding: 10px;\n",
    "font-size: 14px;\n",
    "\">\n",
    "<b>Crash Count Legend:</b><br>\n",
    "<span style=\"color: red;\">●</span> 1 Crash<br>\n",
    "<span style=\"color: purple;\">●</span> 2 Crashes<br>\n",
    "</div>\n",
    "\"\"\"\n",
    "proportional_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "# Save and display the map\n",
    "#proportional_map.save(\"proportional_map_final.html\")  #Uncomment thid line to save the map as a html file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "#display(FileLink(\"proportional_map_final.html\")) #Uncomment this line to display the download link\n",
    "#proportional_map     #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"traffic_analysis_tool\")\n",
    "\n",
    "# Assuming 'defects_valid' is your DataFrame containing the data\n",
    "\n",
    "# Filter for 'WORN SURFACE' road defects\n",
    "area_worn = defects_valid[defects_valid['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "\n",
    "# Group by 'STREET_NAME' and count occurrences\n",
    "worn_street_counts = area_worn['STREET_NAME'].value_counts()\n",
    "\n",
    "# Select the top streets with the highest counts\n",
    "top_worn_streets = worn_street_counts.index.tolist()\n",
    "\n",
    "# Initialize a dictionary to store differences by ZIP code\n",
    "zip_code_differences = {}\n",
    "\n",
    "# Iterate over the top streets\n",
    "for street in top_worn_streets:\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    street_data['CRASH_YEAR'] = street_data['CRASH_DATE'].dt.year\n",
    "    \n",
    "    # Group by year and count crashes\n",
    "    street_g = street_data.groupby('CRASH_YEAR').size()\n",
    "    \n",
    "    # Ensure both 2023 and 2024 exist in the data\n",
    "    if 2023 in street_g.index and 2024 in street_g.index:\n",
    "        difference = street_g.loc[2024] - street_g.loc[2023]\n",
    "        if difference > 0:\n",
    "            # Get the mean latitude and longitude for the street\n",
    "            lat = street_data['LATITUDE'].mean()\n",
    "            lon = street_data['LONGITUDE'].mean()\n",
    "            \n",
    "            # Introduce a delay to prevent rate-limiting\n",
    "            try:\n",
    "                location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "                zip_code = location.raw['address'].get('postcode', 'Unknown')\n",
    "                time.sleep(1)  # Delay of 1 second\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving ZIP code for {street}: {e}\")\n",
    "                zip_code = 'Unknown'\n",
    "            \n",
    "            # Aggregate differences by ZIP code\n",
    "            if zip_code != 'Unknown':\n",
    "                zip_code_differences[zip_code] = zip_code_differences.get(zip_code, 0) + difference\n",
    "\n",
    "# Create a Geo Map for ZIP codes with positive differences\n",
    "geo_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)  # Centered on Chicago (example location)\n",
    "\n",
    "# Add markers for ZIP codes with positive differences\n",
    "for zip_code, difference in zip_code_differences.items():\n",
    "    # Use the ZIP code to get a representative location\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{zip_code}, Chicago, IL\")\n",
    "        if location:\n",
    "            folium.Marker(\n",
    "                location=[location.latitude, location.longitude],\n",
    "                popup=f\"ZIP Code: {zip_code}\\n{difference} more crashes in 2024\",\n",
    "                icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "            ).add_to(geo_map)\n",
    "            time.sleep(1)  # Delay of 1 second to prevent rate-limiting\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving location for ZIP code {zip_code}: {e}\")\n",
    "\n",
    "# Save the map to an HTML file\n",
    "#geo_map.save(\"zip_code_crash_differences.html\") #Uncomment thid line to save the map as a html file\n",
    "\n",
    "# Provide a download link for the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"zip_code_crash_differences.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#geo_map      #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = []\n",
    "\n",
    "# Iterate over the top streets\n",
    "for street in top_worn_streets:\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    street_data['CRASH_YEAR'] = street_data['CRASH_DATE'].dt.year\n",
    "    \n",
    "    # Group by year and count crashes\n",
    "    street_g = street_data.groupby('CRASH_YEAR').size()\n",
    "    \n",
    "    # Ensure both 2023 and 2024 exist in the data\n",
    "    if 2023 in street_g.index and 2024 in street_g.index:\n",
    "        difference = street_g.loc[2024] - street_g.loc[2023]\n",
    "        if difference > 0:\n",
    "            # Get the mean latitude and longitude for the street\n",
    "            lat = street_data['LATITUDE'].mean()\n",
    "            lon = street_data['LONGITUDE'].mean()\n",
    "            \n",
    "            # Append the data for the heatmap (convert to float)\n",
    "            heatmap_data.append([float(lat), float(lon), float(difference)])\n",
    "heatmap_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)  # Centered on Chicago (example location)\n",
    "\n",
    "# Add the heat map layer\n",
    "HeatMap(\n",
    "    data=heatmap_data,\n",
    "    radius=15,  # Size of each point on the heatmap\n",
    "    blur=10,    # Blurring to smooth out the visualization\n",
    "    max_zoom=12\n",
    ").add_to(heatmap_map)\n",
    "\n",
    "#heatmap_map.save(\"heatmap_crash_differences.html\") #Uncomment thid line to save the map as a html file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"heatmap_crash_differences.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#heatmap_map     #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Choropleth\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the converted GeoJSON file\n",
    "geojson_path = \"C:/Users/byash/OneDrive/Desktop/TDSP/Zip_Codes_20250127.geojson\"\n",
    "\n",
    "boundary_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "# Add the Choropleth layer\n",
    "Choropleth(\n",
    "    geo_data=geojson_path,\n",
    "    data=pd.DataFrame(list(zip_code_differences.items()), columns=['ZIP', 'DIFFERENCE']),\n",
    "    columns=['ZIP', 'DIFFERENCE'],\n",
    "    key_on=\"feature.properties.ZIP\",\n",
    "    fill_color='Reds',  \n",
    "    fill_opacity=0.8,  \n",
    "    line_opacity=0.4, \n",
    "    legend_name='Crash Differences (2024 - 2023)'\n",
    ").add_to(boundary_map)\n",
    "\n",
    "#boundary_map      #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Coordinates of the two endpoints of the line\n",
    "point1 = (41.81392, -87.76234)  \n",
    "point2 = (41.86721, -87.5938)  \n",
    "\n",
    "# Calculate slope (m) and intercept (c)\n",
    "x1, y1 = point1[1], point1[0]\n",
    "x2, y2 = point2[1], point2[0]\n",
    "m = (y2 - y1) / (x2 - x1)\n",
    "c = y1 - m * x1\n",
    "\n",
    "# Define a function to determine the position relative to the line\n",
    "def assign_group(row):\n",
    "    y_line = m * row['LONGITUDE'] + c\n",
    "    if row['LATITUDE'] > y_line:\n",
    "        return 'Above'\n",
    "    else:\n",
    "        return 'Below'\n",
    "\n",
    "# Apply this to your dataset\n",
    "defects_valid['Group'] = defects_valid.apply(assign_group, axis=1)\n",
    "\n",
    "# Separate the data into two groups\n",
    "above_data = defects_valid[defects_valid['Group'] == 'Above']\n",
    "below_data = defects_valid[defects_valid['Group'] == 'Below']\n",
    "\n",
    "# Output the results\n",
    "print(\"Number of data points above the line:\", len(above_data))\n",
    "print(\"Number of data points below the line:\", len(below_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Define the center of the map (e.g., the median latitude and longitude of the data)\n",
    "map_center_lat = defects_valid['LATITUDE'].median()\n",
    "map_center_lon = defects_valid['LONGITUDE'].median()\n",
    "\n",
    "# Create a Folium map centered on the data\n",
    "map_defects = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12)\n",
    "\n",
    "# Add markers for each point in defects_valid\n",
    "for _, row in defects_valid.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        radius=3,\n",
    "        color='blue',  \n",
    "        fill=True,\n",
    "        fill_color='red',  \n",
    "        fill_opacity=0.7,\n",
    "        tooltip=f\"Street: {row['STREET_NAME']}<br>Defect: {row['ROAD_DEFECT']}\"\n",
    "    ).add_to(map_defects)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "#map_defects.save(\"defects_valid_points_map.html\") #Uncomment thid line to save the map as a html file\n",
    "\n",
    "# Provide a link to download or view the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"defects_valid_points_map.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#map_defects    #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "below_data_lat = below_data['LATITUDE'].mean()\n",
    "below_data_lon = below_data['LONGITUDE'].mean()\n",
    "below_data_map = folium.Map(location=[below_data_lat, below_data_lon], zoom_start=12)\n",
    "\n",
    "# Add markers for each point in below_data\n",
    "for _, row in below_data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        radius=3,\n",
    "        color='blue',  \n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        tooltip=f\"Street: {row['STREET_NAME']}<br>Defect: {row['ROAD_DEFECT']}\"\n",
    "    ).add_to(below_data_map)\n",
    "\n",
    "#below_data_map.save(\"below_data_map.html\") #Uncomment thid line to save the map as a html file\n",
    "\n",
    "# Provide a link to download or view the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"below_data_map.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#below_data_map   #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_data_d= below_data[['ROAD_DEFECT','CRASH_DATE']]\n",
    "below_data_d['CRASH_DATE'] = pd.to_datetime(below_data_d['CRASH_DATE']).dt.year\n",
    "below_data_d\n",
    "\n",
    "above_data_d= above_data[['ROAD_DEFECT','CRASH_DATE']]\n",
    "above_data_d['CRASH_DATE'] = pd.to_datetime(above_data_d['CRASH_DATE']).dt.year\n",
    "above_data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by crash date and count for below_data_d\n",
    "below_data_dy = below_data_d.groupby('CRASH_DATE').count()\n",
    "\n",
    "# Group by crash date and count for above_data_d\n",
    "above_data_dy = above_data_d.groupby('CRASH_DATE').count()\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8, 6), sharex=True)\n",
    "\n",
    "# Plot for below_data_d\n",
    "ax[0].plot(below_data_dy.index, below_data_dy['ROAD_DEFECT'], marker='o', color='blue', label='Below Line')\n",
    "ax[0].set_title('Crashes Below Line', fontsize=10)\n",
    "ax[0].set_ylabel('Crashes', fontsize=9)\n",
    "ax[0].legend(fontsize=8)\n",
    "ax[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot for above_data_d\n",
    "ax[1].plot(above_data_dy.index, above_data_dy['ROAD_DEFECT'], marker='o', color='green', label='Above Line')\n",
    "ax[1].set_title('Crashes Above Line', fontsize=10)\n",
    "ax[1].set_xlabel('Years', fontsize=9)\n",
    "ax[1].set_ylabel('Crashes', fontsize=9)\n",
    "ax[1].legend(fontsize=8)\n",
    "ax[1].grid(alpha=0.3)\n",
    "\n",
    "# Adjust layout for compactness\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_data_worn = below_data_d[below_data_d['ROAD_DEFECT']=='WORN SURFACE']\n",
    "below_data_worn.reset_index(drop=True,inplace=True)\n",
    "below_data_worn_y = below_data_worn.groupby('CRASH_DATE').count()\n",
    "below_data_worn_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_data_dy = above_data_d.groupby('CRASH_DATE').count()\n",
    "above_data_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above_data_worn = above_data_d[above_data_d['ROAD_DEFECT']=='RUT, HOLES']\n",
    "above_data_worn.reset_index(drop=True,inplace=True)\n",
    "above_data_worn_y = above_data_worn.groupby('CRASH_DATE').count()\n",
    "above_data_worn_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUT, HOLES\n",
    "below_data_holes = below_data_d[below_data_d['ROAD_DEFECT'] == 'RUT, HOLES']\n",
    "below_data_holes.reset_index(drop=True, inplace=True)\n",
    "below_data_holes_y = below_data_holes.groupby('CRASH_DATE').count()\n",
    "below_data_holes_y = pd.DataFrame(below_data_holes_y)\n",
    "\n",
    "# WORN SURFACE\n",
    "below_data_worn = below_data_d[below_data_d['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "below_data_worn.reset_index(drop=True, inplace=True)\n",
    "below_data_worn_y = below_data_worn.groupby('CRASH_DATE').count()\n",
    "below_data_worn_y = pd.DataFrame(below_data_worn_y)\n",
    "\n",
    "# DEBRIS ON ROADWAY\n",
    "below_data_deb = below_data_d[below_data_d['ROAD_DEFECT'] == 'DEBRIS ON ROADWAY']\n",
    "below_data_deb.reset_index(drop=True, inplace=True)\n",
    "below_data_deb_y = below_data_deb.groupby('CRASH_DATE').count()\n",
    "below_data_deb_y = pd.DataFrame(below_data_deb_y)\n",
    "\n",
    "# SHOULDER DEFECT\n",
    "below_data_sh = below_data_d[below_data_d['ROAD_DEFECT'] == 'SHOULDER DEFECT']\n",
    "below_data_sh.reset_index(drop=True, inplace=True)\n",
    "below_data_sh_y = below_data_sh.groupby('CRASH_DATE').count()\n",
    "below_data_sh_y = pd.DataFrame(below_data_sh_y)\n",
    "\n",
    "# PLOT\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 8))\n",
    "ax[0, 0].plot(below_data_holes_y.index, below_data_holes_y['ROAD_DEFECT'], color='red', marker='o')\n",
    "ax[0, 0].set_title('Pot holes')\n",
    "ax[0, 0].set_ylabel('Crashes')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "# Plot on the second subplot (axs[1])\n",
    "ax[0, 1].plot(below_data_worn_y.index, below_data_worn_y['ROAD_DEFECT'], color='orange', marker='o')\n",
    "ax[0, 1].set_title('Worn surfaces')\n",
    "ax[0, 1].set_ylabel('Crashes')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[1, 0].plot(below_data_deb_y.index, below_data_deb_y['ROAD_DEFECT'], color='green', marker='o')\n",
    "ax[1, 0].set_title('Debris on road')\n",
    "ax[1, 0].set_xlabel('Years')\n",
    "ax[1, 0].set_ylabel('Crashes')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "# Plot on the fourth subplot (axs[1,1])\n",
    "ax[1, 1].plot(below_data_sh_y.index, below_data_sh_y['ROAD_DEFECT'], marker='o')\n",
    "ax[1, 1].set_title('Shoulder defect')\n",
    "ax[1, 1].set_xlabel('Years')\n",
    "ax[1, 1].set_ylabel('Crashes')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUT, HOLES\n",
    "above_data_holes = above_data_d[above_data_d['ROAD_DEFECT'] == 'RUT, HOLES']\n",
    "above_data_holes.reset_index(drop=True, inplace=True)\n",
    "above_data_holes_y = above_data_holes.groupby('CRASH_DATE').count()\n",
    "above_data_holes_y = pd.DataFrame(above_data_holes_y)\n",
    "\n",
    "# WORN SURFACE\n",
    "above_data_worn = above_data_d[above_data_d['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "above_data_worn.reset_index(drop=True, inplace=True)\n",
    "above_data_worn_y = above_data_worn.groupby('CRASH_DATE').count()\n",
    "above_data_worn_y = pd.DataFrame(above_data_worn_y)\n",
    "\n",
    "# DEBRIS ON ROADWAY\n",
    "above_data_deb = above_data_d[above_data_d['ROAD_DEFECT'] == 'DEBRIS ON ROADWAY']\n",
    "above_data_deb.reset_index(drop=True, inplace=True)\n",
    "above_data_deb_y = above_data_deb.groupby('CRASH_DATE').count()\n",
    "above_data_deb_y = pd.DataFrame(above_data_deb_y)\n",
    "\n",
    "# SHOULDER DEFECT\n",
    "above_data_sh = above_data_d[above_data_d['ROAD_DEFECT'] == 'SHOULDER DEFECT']\n",
    "above_data_sh.reset_index(drop=True, inplace=True)\n",
    "above_data_sh_y = above_data_sh.groupby('CRASH_DATE').count()\n",
    "above_data_sh_y = pd.DataFrame(above_data_sh_y)\n",
    "\n",
    "# PLOT\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 8))\n",
    "ax[0, 0].plot(above_data_holes_y.index, above_data_holes_y['ROAD_DEFECT'], color='red', marker='o')\n",
    "ax[0, 0].set_title('Pot holes')\n",
    "ax[0, 0].set_ylabel('Crashes')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "# Plot on the second subplot (axs[1])\n",
    "ax[0, 1].plot(above_data_worn_y.index, above_data_worn_y['ROAD_DEFECT'], color='orange', marker='o')\n",
    "ax[0, 1].set_title('Worn surfaces')\n",
    "ax[0, 1].set_ylabel('Crashes')\n",
    "ax[0, 1].legend()\n",
    "\n",
    "ax[1, 0].plot(above_data_deb_y.index, above_data_deb_y['ROAD_DEFECT'], color='green', marker='o')\n",
    "ax[1, 0].set_title('Debris on road')\n",
    "ax[1, 0].set_xlabel('Years')\n",
    "ax[1, 0].set_ylabel('Crashes')\n",
    "ax[1, 0].legend()\n",
    "\n",
    "# Plot on the fourth subplot (axs[1,1])\n",
    "ax[1, 1].plot(above_data_sh_y.index, above_data_sh_y['ROAD_DEFECT'], marker='o')\n",
    "ax[1, 1].set_title('Shoulder defect')\n",
    "ax[1, 1].set_xlabel('Years')\n",
    "ax[1, 1].set_ylabel('Crashes')\n",
    "ax[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'WORN SURFACE' road defects\n",
    "area_worn = below_data[below_data['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "\n",
    "# Group by 'STREET_NAME' and count occurrences\n",
    "worn_street_counts = area_worn['STREET_NAME'].value_counts()\n",
    "\n",
    "# Select the top streets\n",
    "top_worn_streets = worn_street_counts.index.tolist()\n",
    "\n",
    "# Initialize a list for streets with increasing crashes from 2023 to 2024\n",
    "increasing_streets = []\n",
    "\n",
    "# Iterate over the top streets\n",
    "for street in top_worn_streets:\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    street_data['CRASH_YEAR'] = street_data['CRASH_DATE'].dt.year\n",
    "    \n",
    "    # Group by year and count crashes\n",
    "    street_g = street_data.groupby('CRASH_YEAR').size()\n",
    "    \n",
    "    # Ensure both 2023 and 2024 exist in the data\n",
    "    if 2023 in street_g.index and 2024 in street_g.index:\n",
    "        if street_g.loc[2024] > street_g.loc[2023]:  # Check for increase\n",
    "            increasing_streets.append(street)\n",
    "\n",
    "# Create subplots for increasing streets\n",
    "num_streets = len(increasing_streets)\n",
    "num_cols = 2\n",
    "num_rows = (num_streets + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows), sharex=True)\n",
    "axes = axes.flatten()  # Flatten in case of a 2D array\n",
    "\n",
    "# Plot the data for streets with increasing crashes\n",
    "for i, (street, ax) in enumerate(zip(increasing_streets, axes)):\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE']).dt.year\n",
    "    street_g = street_data.groupby('CRASH_DATE').size()\n",
    "\n",
    "    ax.plot(street_g.index, street_g.values, marker='o', linestyle='-')\n",
    "    ax.set_title(f'Crashes on {street} Over Time')\n",
    "    ax.set_xlabel('Crash Date')\n",
    "    ax.set_ylabel('Number of Crashes')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a Geo Map for streets with positive differences with the default background (OpenStreetMap)\n",
    "geo_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11, tiles='OpenStreetMap')\n",
    "\n",
    "# Add proportional circles for streets with positive differences\n",
    "for street, difference in positive_differences.items():\n",
    "    # Filter the location data for the street for 2024\n",
    "    location_data_2024 = area_worn[\n",
    "        (area_worn['STREET_NAME'] == street) & \n",
    "        (area_worn['CRASH_DATE'].dt.year == 2024)\n",
    "    ]\n",
    "    \n",
    "    # Use the median latitude and longitude for better accuracy\n",
    "    lat = location_data_2024['LATITUDE'].median()\n",
    "    lon = location_data_2024['LONGITUDE'].median()\n",
    "    \n",
    "    # Add a proportional circle marker to the map\n",
    "    folium.CircleMarker(\n",
    "        location=[lat, lon],\n",
    "        radius=int(difference) * 2,  # Convert difference to a standard int\n",
    "        tooltip=folium.Tooltip(f\"<b>Street:</b> {street}<br><b>Crash Increase:</b> {difference}\"),  # Styled tooltip with bold text\n",
    "        popup=folium.Popup(f\"<b>Street:</b> {street}<br><b>Crash Increase:</b> {difference}<br><b>Year:</b> 2024\", max_width=300),\n",
    "        color='darkblue',\n",
    "        fill=True,\n",
    "        fill_color='orange',  # Use a contrasting fill color for better visibility\n",
    "        fill_opacity=0.8\n",
    "    ).add_to(geo_map)\n",
    "\n",
    "# Add a legend using an HTML element\n",
    "legend_html = \"\"\"\n",
    "<div style=\"\n",
    "position: fixed;\n",
    "bottom: 50px;\n",
    "left: 50px;\n",
    "width: 250px;\n",
    "background-color: white;\n",
    "border: 2px solid black;\n",
    "z-index: 1000;\n",
    "padding: 10px;\n",
    "font-size: 14px;\n",
    "box-shadow: 5px 5px 5px rgba(0,0,0,0.3);\n",
    "\">\n",
    "<b>Legend:</b><br>\n",
    "<span style=\"color: darkblue;\">●</span> <b>Proportional circles:</b> Crash increase in 2024.<br>\n",
    "Circle size indicates the magnitude of the increase.\n",
    "</div>\n",
    "\"\"\"\n",
    "geo_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map to an HTML file or display it\n",
    "#geo_map.save(\"default_background_crash_map.html\") #Uncomment thid line to save the map as a html file\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Provide a download link for the HTML file\n",
    "#display(FileLink(\"default_background_crash_map.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#geo_map    #Uncomment this line to display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'CRASH_DATE' is in datetime format\n",
    "below_data['CRASH_DATE'] = pd.to_datetime(below_data['CRASH_DATE'])\n",
    "\n",
    "# Filter for streets in the 'name' list and for the year 2024\n",
    "filtered_2024_data = below_data[\n",
    "    (below_data['STREET_NAME'].isin(name.keys())) &\n",
    "    (below_data['CRASH_DATE'].dt.year == 2024)\n",
    "]\n",
    "\n",
    "filtered_2024_data_worn = filtered_2024_data[filtered_2024_data['ROAD_DEFECT']=='WORN SURFACE']\n",
    "filtered_2024_data_worn.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "\n",
    "# Define the center of the map (e.g., Chicago) based on the mean latitude and longitude\n",
    "map_center_lat = filtered_2024_data_worn['LATITUDE'].mean()\n",
    "map_center_lon = filtered_2024_data_worn['LONGITUDE'].mean()\n",
    "\n",
    "# Create a folium map centered on the data\n",
    "heat_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12)\n",
    "\n",
    "# Prepare data for the heat map\n",
    "heat_data = filtered_2024_data_worn[['LATITUDE', 'LONGITUDE']].values.tolist()\n",
    "\n",
    "# Add a heat map layer\n",
    "HeatMap(heat_data).add_to(heat_map)\n",
    "\n",
    "# Save the heat map to an HTML file\n",
    "#heat_map.save(\"filtered_worn_surface_2024_heatmap.html\")  #Uncomment thid line to save the map as a html file\n",
    "\n",
    "# Provide a link to download the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"filtered_worn_surface_2024_heatmap.html\")) #Uncomment this line to display the download link\n",
    "\n",
    "#heat_map   #Uncomment this line to display the map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered at the calculated mean latitude and longitude\n",
    "proportional_map = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12)\n",
    "\n",
    "# Ensure 'CRASH_COUNT' column exists\n",
    "filtered_2024_data_worn['CRASH_COUNT'] = 1\n",
    "\n",
    "# Group by coordinates and calculate crash counts\n",
    "crash_counts = filtered_2024_data_worn.groupby(['LATITUDE', 'LONGITUDE']).size().reset_index(name='CRASH_COUNT')\n",
    "\n",
    "# Function to determine marker color based on crash count\n",
    "# Ensure CRASH_COUNT is numeric\n",
    "crash_counts['CRASH_COUNT'] = crash_counts['CRASH_COUNT'].astype(int)\n",
    "\n",
    "# Define a function to determine the marker color\n",
    "def get_color(crash_count):\n",
    "    if crash_count == 1:\n",
    "        return 'red'  \n",
    "    elif crash_count == 2:\n",
    "        return 'blue'  \n",
    "    return 'gray'  \n",
    "\n",
    "# Create proportional markers with dynamic colors\n",
    "for _, row in crash_counts.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "        radius=row['CRASH_COUNT'] * 5,  \n",
    "        popup=f\"<b>Crashes:</b> {row['CRASH_COUNT']}\", \n",
    "        tooltip=f\"Crashes: {row['CRASH_COUNT']}\",  \n",
    "        color='black', \n",
    "        fill=True,\n",
    "        fill_color=get_color(row['CRASH_COUNT']), \n",
    "        fill_opacity=0.8,  \n",
    "    ).add_to(proportional_map)\n",
    "legend_html = \"\"\"\n",
    "<div style=\"\n",
    "position: fixed;\n",
    "bottom: 50px;\n",
    "left: 50px;\n",
    "width: 350px;\n",
    "background-color: white;\n",
    "border: 2px solid black;\n",
    "z-index: 1000;\n",
    "padding: 10px;\n",
    "font-size: 14px;\n",
    "\">\n",
    "<b>Crash Count Legend:</b><br>\n",
    "<span style=\"color: red;\">●</span> 1 Crash<br>\n",
    "<span style=\"color: purple;\">●</span> 2 Crashes<br>\n",
    "</div>\n",
    "\"\"\"\n",
    "proportional_map.get_root().html.add_child(folium.Element(legend_html))\n",
    "# Save and display the map\n",
    "#proportional_map.save(\"proportional_map_final.html\")\n",
    "from IPython.display import FileLink\n",
    "\n",
    "#display(FileLink(\"proportional_map_final.html\"))\n",
    "#proportional_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"traffic_analysis_tool\")\n",
    "\n",
    "# Filter for 'WORN SURFACE' road defects\n",
    "area_worn = below_data[below_data['ROAD_DEFECT'] == 'WORN SURFACE']\n",
    "\n",
    "# Group by 'STREET_NAME' and count occurrences\n",
    "worn_street_counts = area_worn['STREET_NAME'].value_counts()\n",
    "\n",
    "# Select the top streets with the highest counts\n",
    "top_worn_streets = worn_street_counts.index.tolist()\n",
    "\n",
    "# Initialize a dictionary to store differences by ZIP code\n",
    "zip_code_differences = {}\n",
    "\n",
    "# Iterate over the top streets\n",
    "for street in top_worn_streets:\n",
    "    # Filter data for the current street\n",
    "    street_data = area_worn[area_worn['STREET_NAME'] == street]\n",
    "    \n",
    "    # Convert 'CRASH_DATE' to datetime format\n",
    "    street_data['CRASH_DATE'] = pd.to_datetime(street_data['CRASH_DATE'])\n",
    "    street_data['CRASH_YEAR'] = street_data['CRASH_DATE'].dt.year\n",
    "    \n",
    "    # Group by year and count crashes\n",
    "    street_g = street_data.groupby('CRASH_YEAR').size()\n",
    "    \n",
    "    # Ensure both 2023 and 2024 exist in the data\n",
    "    if 2023 in street_g.index and 2024 in street_g.index:\n",
    "        difference = street_g.loc[2024] - street_g.loc[2023]\n",
    "        if difference > 0:\n",
    "            # Get the mean latitude and longitude for the street\n",
    "            lat = street_data['LATITUDE'].mean()\n",
    "            lon = street_data['LONGITUDE'].mean()\n",
    "            \n",
    "            # Introduce a delay to prevent rate-limiting\n",
    "            try:\n",
    "                location = geolocator.reverse((lat, lon), exactly_one=True)\n",
    "                zip_code = location.raw['address'].get('postcode', 'Unknown')\n",
    "                time.sleep(1)  # Delay of 1 second\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving ZIP code for {street}: {e}\")\n",
    "                zip_code = 'Unknown'\n",
    "            \n",
    "            # Aggregate differences by ZIP code\n",
    "            if zip_code != 'Unknown':\n",
    "                zip_code_differences[zip_code] = zip_code_differences.get(zip_code, 0) + difference\n",
    "\n",
    "# Create a Geo Map for ZIP codes with positive differences\n",
    "geo_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)  # Centered on Chicago (example location)\n",
    "\n",
    "# Add markers for ZIP codes with positive differences\n",
    "for zip_code, difference in zip_code_differences.items():\n",
    "    # Use the ZIP code to get a representative location\n",
    "    try:\n",
    "        location = geolocator.geocode(f\"{zip_code}, Chicago, IL\")\n",
    "        if location:\n",
    "            folium.Marker(\n",
    "                location=[location.latitude, location.longitude],\n",
    "                popup=f\"ZIP Code: {zip_code}\\n{difference} more crashes in 2024\",\n",
    "                icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n",
    "            ).add_to(geo_map)\n",
    "            time.sleep(1)  # Delay of 1 second to prevent rate-limiting\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving location for ZIP code {zip_code}: {e}\")\n",
    "\n",
    "# Save the map to an HTML file\n",
    "#geo_map.save(\"zip_code_crash_differences.html\")\n",
    "\n",
    "# Provide a download link for the HTML file\n",
    "from IPython.display import FileLink\n",
    "#display(FileLink(\"zip_code_crash_differences.html\"))\n",
    "\n",
    "#geo_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Choropleth\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the converted GeoJSON file\n",
    "geojson_path = \"C:/Users/byash/OneDrive/Desktop/TDSP/Zip_Codes_20250127.geojson\"\n",
    "\n",
    "# Create a map\n",
    "boundary_map = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "\n",
    "# Add the Choropleth layer\n",
    "Choropleth(\n",
    "    geo_data=geojson_path,\n",
    "    data=pd.DataFrame(list(zip_code_differences.items()), columns=['ZIP', 'DIFFERENCE']),\n",
    "    columns=['ZIP', 'DIFFERENCE'],\n",
    "    key_on=\"feature.properties.ZIP\",  # Match this to the GeoJSON property for ZIP\n",
    "    fill_color='Reds',  # Use shades of red\n",
    "    fill_opacity=0.8,  # Increase fill opacity\n",
    "    line_opacity=0.4,  # Slightly thicker boundary lines\n",
    "    legend_name='Crash Differences (2024 - 2023)'\n",
    ").add_to(boundary_map)\n",
    "\n",
    "# Display the map directly in the notebook\n",
    "#boundary_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_file(filename, search_path):\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "# Define the filename and the search path (current directory)\n",
    "filename = 'map_with_street_name.html'\n",
    "search_path = '.'\n",
    "\n",
    "# Find the file\n",
    "file_path = find_file(filename, search_path)\n",
    "\n",
    "if file_path:\n",
    "    print(f\"File found: {file_path}\")\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
